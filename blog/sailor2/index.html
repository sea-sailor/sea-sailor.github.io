<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs | Sailor</title>
<meta name=keywords content><meta name=description content="GITHUB
HUGGING FACE
DEMO
COMMUNITY
Introduction
In this blog, we introduce Sailor2, a community-driven initiative that brings cutting-edge multilingual language models to South-East Asia (SEA). Our research highlights a strong demand for models in the 8B and 20B parameter range for production use, alongside a 1B model for specialized applications, such as speculative decoding and research purposes. These models, released under the Apache 2.0 license, provide enhanced accessibility to advanced language technologies across the region."><meta name=author content="Sailor Team"><link rel=canonical href=http://sea-sailor.github.io/blog/sailor2/><link crossorigin=anonymous href=/assets/css/stylesheet.5535818ba8080a84fafce624766afd4112e9f8489b7a167457c88f8ea1398972.css integrity="sha256-VTWBi6gICoT6/OYkdmr9QRLp+EibehZ0V8iPjqE5iXI=" rel="preload stylesheet" as=style><link rel=icon href=http://sea-sailor.github.io/img/sailor_logo_trans.png><link rel=icon type=image/png sizes=16x16 href=http://sea-sailor.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://sea-sailor.github.io/favicon-32x32.png><link rel=apple-touch-icon href=http://sea-sailor.github.io/apple-touch-icon.png><link rel=mask-icon href=http://sea-sailor.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://sea-sailor.github.io/blog/sailor2/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BP9G7L138H"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BP9G7L138H")}</script><meta property="og:title" content="Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs"><meta property="og:description" content="GITHUB
HUGGING FACE
DEMO
COMMUNITY
Introduction
In this blog, we introduce Sailor2, a community-driven initiative that brings cutting-edge multilingual language models to South-East Asia (SEA). Our research highlights a strong demand for models in the 8B and 20B parameter range for production use, alongside a 1B model for specialized applications, such as speculative decoding and research purposes. These models, released under the Apache 2.0 license, provide enhanced accessibility to advanced language technologies across the region."><meta property="og:type" content="article"><meta property="og:url" content="http://sea-sailor.github.io/blog/sailor2/"><meta property="og:image" content="http://sea-sailor.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-12-02T12:00:00+08:00"><meta property="article:modified_time" content="2024-12-02T12:00:00+08:00"><meta property="og:site_name" content="Sailor"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://sea-sailor.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs"><meta name=twitter:description content="GITHUB
HUGGING FACE
DEMO
COMMUNITY
Introduction
In this blog, we introduce Sailor2, a community-driven initiative that brings cutting-edge multilingual language models to South-East Asia (SEA). Our research highlights a strong demand for models in the 8B and 20B parameter range for production use, alongside a 1B model for specialized applications, such as speculative decoding and research purposes. These models, released under the Apache 2.0 license, provide enhanced accessibility to advanced language technologies across the region."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"http://sea-sailor.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs","item":"http://sea-sailor.github.io/blog/sailor2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs","name":"Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs","description":"GITHUB HUGGING FACE DEMO COMMUNITY\nIntroduction In this blog, we introduce Sailor2, a community-driven initiative that brings cutting-edge multilingual language models to South-East Asia (SEA). Our research highlights a strong demand for models in the 8B and 20B parameter range for production use, alongside a 1B model for specialized applications, such as speculative decoding and research purposes. These models, released under the Apache 2.0 license, provide enhanced accessibility to advanced language technologies across the region.\n","keywords":[],"articleBody":"GITHUB HUGGING FACE DEMO COMMUNITY\nIntroduction In this blog, we introduce Sailor2, a community-driven initiative that brings cutting-edge multilingual language models to South-East Asia (SEA). Our research highlights a strong demand for models in the 8B and 20B parameter range for production use, alongside a 1B model for specialized applications, such as speculative decoding and research purposes. These models, released under the Apache 2.0 license, provide enhanced accessibility to advanced language technologies across the region.\nSailor2 builds upon the foundation of the awesome multilingual model Qwen2.5 (Learn more here) and is continuously pre-trained on ~500B tokens to support 15 languages better with a unified model. These languages include: English, Chinese, Burmese ðŸ‡²ðŸ‡², CebuanoðŸ‡µðŸ‡­, IlocanoðŸ‡µðŸ‡­, IndonesianðŸ‡®ðŸ‡©, JavaneseðŸ‡®ðŸ‡©, KhmerðŸ‡°ðŸ‡­, LaoðŸ‡±ðŸ‡¦, MalayðŸ‡²ðŸ‡¾, SundaneseðŸ‡®ðŸ‡©, TagalogðŸ‡µðŸ‡­, ThaiðŸ‡¹ðŸ‡­, VietnameseðŸ‡»ðŸ‡³ and WarayðŸ‡µðŸ‡­.\nBy addressing the growing demand for diverse, robust, and accessible language models, Sailor2 seeks to serve the underserved in SEA areas with open, inclusive, and accessible multilingual LLMs.\nPerformance We evaluate our models on several benchmarks, including IndoCutlure, TydiQA, Meta Thai MMLU, M3Exam, VMLU, Tatabahasa, FLORES-200, and XCOPA. The results of TydiQA, M3Exam, FLORES-200 and XCOPA are obtained from our previously released evaluation suite SailCompass. The results of Meta Thai MMLU are from the LightEval, and others are obtained from the evaluation scripts shared by the community members. More evaluation details will be shared in our upcoming paper.\nCompared to other advanced multilingual models like Qwen2.5-32B, Gemma2-27B, Llama3.1-70B, and Aya-Expanse-32B, our flagship 20B model demonstrates comparable or superior performance on languages such as Indonesian, Thai, Vietnamese, Malay, and Javanese. It excels in machine translation tasks and multilingual reasoning, including benchmarks like XCOPA. Most notably, due to the inclusive design of the Sailor2 models, our 20B model significantly outperforms others in extremely low-resource languages, such as M3Exam-Javanese, achieving an improvement of +14.6 over Qwen2.5-32B.\nThe Sailor2-8B model stands out as the best multilingual model for SEA languages in the \u003c 10B category, outperforming all other open-access language models in general.\nWe also evaluated our chat models using the translated WildBench, Sea-WildBench, a challegning benchmark for chat models. As shown, the win rate of Sailor2-20B-Chat against GPT-4o-0806 on SeaWildBench is nearly 50%, demonstrating that Sailor2-20B-Chat performs at a GPT-4o level for local chat scenarios.\nContinual Pre-training Our data is primarily sourced from web HTML documents and publicly available PDF files, covering a diverse range of languages, including English, Chinese, and the above mentioned SEA languages. For English and Chinese datasets, we predominantly utilize existing datasets shared by the community. For SEA languages, we collect the data independently and in collaboration with partners.\nThe dataset for SEA languages continual pre-training, sourcing documents from a variety of materials containing up-to-date knowledge until the end of 2023. To ensure high data quality, we apply several deduplication and cleaning techniques across each source. After comprehensive deduplication and cleaning, the total available dataset is as follows, along with its corresponding disk usage (ordered by disk size):\nLanguage Disk size Vietnamese 1.9T Indonesian 1.3T Thai 242G Malay 44G Burmese 25.8G Tagalog 17.5G Khmer 6.9G Cebuano 2.1G Lao 1.9G Javanese 1.2G Waray 0.8G Sundanese 0.75G Ilocano 0.2G Model Expansion The Sailor2 model comes in three sizes, 1B, 8B, and 20B, which are expanded from the Qwen2.5 base models of 0.5B, 7B, and 14B, respectively. The decision was made to perform model expansion prior to continual pre-training in order to mitigate the potential for forgetting of English and Chinese language capabilities, while also enhancing the modelâ€™s capacity for further improvements in SEA languages.\nIn practice, the approach draws inspiration from the method proposed by LlamaPro, leveraging a block-expansion mechanism in the original Qwen2.5 model. This approach significantly enhances the modelâ€™s performance in SEA languages while maintaining stable capabilities in English and Chinese. By utilizing the strategy, the newly introduced layers are able to store the additional SEA knowledge from the continually pre-trained tokens, rather than overwriting the existing linguistic information of the other languages.\nData Cleaning and Deduplication We leveraged sailcraft for comprehensive data processing, employing multiple deduplication techniques including near and exact deduplication, heuristics-based cleaning, URL deduplication, and frequent line removal. During URL deduplication, we prioritized documents with more content, effectively reducing total tokens by nearly 50%. As for the frequent line removal, following the Llama3 approach, we removed lines appearing more than 5 times in 10M document buckets, successfully eliminating nearly 5% of total tokensâ€”most of which were determined to be meaningless content.\nTwo-Stage Training We adopt the two-stage pre-training approach introduced in the MiniCPM paper. In the first stage, we use comprehensive datasets and a relatively high learning rate (1e-4), while in the second stage, we focus on high-quality tokens with a smaller learning rate (1e-5).\nDrawing from the forgetting rules outlined in the Sailor paper, we introduce high-resource languages during the first stage (such as English, Chinese, Vietnamese, Indonesian, Thai, Malay, Burmese, Tagalog, and Khmer). In the second stage, we transition to both high-resource and low-resource languages (including Cebuano, Lao, Javanese, Waray, Sundanese, and Ilocano). This two-stage approach allows automatic data mixture in the first stage, while allowing us to incorporate high-quality tokens from low-resource languages in the second stage without rescheduling the mixing ratios.\nStage 1: Balanced Data Mixture In the stage 1 of our continual pre-training, we select a subset of languages that could provide sufficiently enough tokens for data mixture optimization. We employed RegMix to optimize the data mixture, with the primary objective of maximizing the log sum across all languages considered in stage 1.\nUnlike our previous practices in Sailor that used 0.5B models as proxy models for data mixture, we follow RegMix to utilize 1M models as our proxy model, even for the scenario of continual pre-training. Our underlying assumption was that if a model can be trained over an extended period, the converged or equivalent data mixture should remain relatively consistent.\nAfter conducting 1,000 runs of data mixture optimization using 1M models, we observed a subtle shift from the original token distribution. Notably, the optimized data mixture resulted in upsampling languages like Khmer, Malay, Burmese, Thai, and Tagalog, while simultaneously downsampling Indonesian and Vietnamese. The final data mixture of Stage 1 is as below (tokens counted in the tokenizer of Qwen2.5):\nLanguage Effective Tokens Vietnamese 102B Indonesian 94B Thai 92B English 51B Chinese 50B Burmese 23.5B Malay 21B Tagalog 10B Khmer 6.5B Stage 1 (Total) 450B Stage 2: Synthetic Data to Retrieve High-Quality Tokens In stage 2, we lower the learning rate to 1e-5 (1/10 of the original learning rate), and take 20% of the stage 1 dataset to make sure the model still behaves well on the original distribution. As for the remaining 80% training budget, we allocate them to high-quality SEA tokens, where all low-resource languages are added, and the token distribution of high-resource languages is maintained as similar to the stage 1.\nTo address challenges in selecting high-quality datasets for low-resource languages, we use NLLB 3.3B to translate high-quality English documents into local languages. A fast-text classifier is then trained for each language to identify high-quality subsets, using 10K positive and 10K negative examples. Positive examples are machine-translated from English high-quality datasets, while negative examples are sampled from the raw dataset from the web. After training, we rank the dataset by classifier scores and select the top results (between 10% to 20%, according to the requirement of the ratio in data mixture) as the high-quality subset for the stage 2 training. In addition, we also added some English instruction tuning datasets and some datasets contributed by the Sailor2 community.\nLanguage Effective Tokens Stage 1 10B English Instruction Tuning Dataset 2.5B Vietnamese (High-Quality) 10.9B Indonesian (High-Quality) 12.8B Thai (High-Quality) 13.9B Burmese (High-Quality) 2.8B Malay (High-Quality) 1.3B Tagalog (High-Quality) 2.2B Khmer (High-Quality) 0.9B Waray (High-Quality) 0.02B Ilocano (High-Quality) 0.05B Javanese (High-Quality) 0.17B Lao (High-Quality) 0.33B Cebuano (High-Quality) 0.30B Sundanese (High-Quality) 0.09B Stage 2 (Total) 60B Post-Training We conduct both the supervised fine-tuning and preference tuning based on the above Sailor2 base models.\nSupervised Fine-tuning Drawing inspiration from OpenCoder, we have implemented a strategic two-stage Supervised Fine-Tuning (SFT) process that tackles common training challenges head-on. In the first stage, we cast a wide net, training the model on the entire dataset for a single epoch using large batch sizes. This initial exposure helps establish broad coverage across different SEA languages and domains.\nThe second stage switches to a more focused approach with smaller batch sizes and multiple training epochs on carefully curated, high-quality data that is balanced across domains and languages. Our quality selection process relies on two key metrics: perplexity from the Sailor2 base model (PPL) and reward scores. High-perplexity examples represent cases where the model is less confident â€“ precisely the areas where it needs the most improvement. While high perplexity identifies learning opportunities, high reward scores ensure weâ€™re learning from exemplary content.\nTo prevent the model from overfitting to similar examples, we employ embedding-based similarity checks. This additional filter ensures our training data remains diverse and representative, leading to a more robust and versatile model. This refined approach to fine-tuning has proven effective in producing models that are both broadly capable and specifically strong in high-priority domains.\nPreference Tuning Our preference tuning consists of two stages, training with off-policy responses generated by Llama-3-8B-Instruct and training with on-policy responses generated by Sailor2 suite. In the off-policy training stage, we first translate the preference dataset, UF-Llama3, into SEA languages. Low-quality translations are then filtered based on perplexity scores provided by the Sailor2-8B base. The resulting off-policy dataset is a mixture of SEA languages and English. In the on-policy training stage, we utilize the prompts from the off-policy dataset. For each prompt, responses are generated using the model trained on the off-policy data. These responses are rated by the open-source reward model, Skywork-Reward-Gemma-2-27B. The highest-rated response is selected as the chosen response and the lowest-rated response is selected as the rejected response. Additionally, we employ a language-consistency verifier to detect and correct cases where the input language differs from the output language, except for the translation task.\nDue to the absence of a high-quality reward model for PPO-based algorithms, we explore different direct alignment algorithms, such as DPO and its variants. We conducted extensive hyperparameter tuning and ablation studies to ensure the model performance on the development evaluations. All experiments were conducted with the training framework, Oat, which enables large-scale and flexible training. Below, we summarize our key findings:\nDPO achieves comparable performance to its length-normalized variants (e.g. Length-normalized DPO and SimPO) while demonstrating better training stability. Off-policy training serves as an effective initialization for on-policy training. On-policy training further improves the model performance across various benchmarks. The dataset processed using the language-consistency verifier provides a consistent performance gain. Develop with Sailor2 The code of Sailor2 has been in the latest Hugging face transformers and we advise you to install transformers==4.46.3.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer device = \"cuda\" # the device to load the model model = AutoModelForCausalLM.from_pretrained(\"sail/Sailor2-20B\", device_map=\"auto\") tokenizer = AutoTokenizer.from_pretrained(\"sail/Sailor2-20B\") input_message = \"Model bahasa adalah model probabilistik\" # The given Indonesian input translates to 'A language model is a probabilistic model of.' model_inputs = tokenizer([input_message], return_tensors=\"pt\").to(device) generated_ids = model.generate( model_inputs.input_ids, max_new_tokens=64 ) generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] print(response) Limited Free API Service We also offer a limited free API service for Sailor2-20B-Chat in collaboration with Float16.cloud. You can access the API using the following details:\ncurl -X POST \"https://api.float16.cloud/v1/chat/completions\" \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer sailor2\" \\ -d '{ \"model\": \"sailor2-20b-chat\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are an AI assistant named Sailor2, created by Sea AI Lab. As an AI assistant, you can answer questions in English, Chinese, and Southeast Asian languages such as Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. Your responses should be friendly, unbiased, informative, detailed, and faithful.\" }, { \"role\": \"user\", \"content\": \"à¸ªà¸§à¸±à¸ªà¸”à¸µ\" } ] }' Acknowledgement This is a huge effort from the collaboration among the community, along with researchers from Sea AI Lab, SCB10X, WiseSight, HuggingFace, and the whole Sailor2 community.\nLimitations and Next Steps Currently, Sailor2 supports a context length of up to 4K tokens. Extending support for longer context lengths, particularly for multilingual use cases, remains a challenge. We are actively working on improvements and plan to support longer contexts in the near future.\nAdditionally, due to the complexity of handling multiple languages simultaneously, Sailor2 may switch languages during responses, especially for smaller ones. We will investigate the problems further.\n","wordCount":"2077","inLanguage":"en","datePublished":"2024-12-02T12:00:00+08:00","dateModified":"2024-12-02T12:00:00+08:00","author":{"@type":"Person","name":"Sailor Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://sea-sailor.github.io/blog/sailor2/"},"publisher":{"@type":"Organization","name":"Sailor","logo":{"@type":"ImageObject","url":"http://sea-sailor.github.io/img/sailor_logo_trans.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Sailor (Alt + H)"></a><div class=logo-switches></div></div><ul id=menu><li><a href=/ title=Home><span>Home</span></a></li><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs</h1><div class=post-meta><span title='2024-12-02 12:00:00 +0800 +0800'>December 2, 2024</span>&nbsp;Â·&nbsp;10 min&nbsp;Â·&nbsp;2077 words&nbsp;Â·&nbsp;Sailor Team</div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/sail-sg/sailor2 class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/collections/sail/sailor2-language-models-674d7c9e6b4dbbd9a869906b class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://huggingface.co/spaces/sail/Sailor2-20B-Chat class="btn external" target=_blank>DEMO</a>
<a href=https://huggingface.co/sailor2 class="btn external" target=_blank>COMMUNITY</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>In this blog, we introduce <strong>Sailor2</strong>, a community-driven initiative that brings cutting-edge multilingual language models to <strong>South-East Asia (SEA)</strong>. Our research highlights a strong demand for models in the <strong>8B</strong> and <strong>20B</strong> parameter range for production use, alongside a <strong>1B</strong> model for specialized applications, such as speculative decoding and research purposes. These models, released under the <strong>Apache 2.0 license</strong>, provide enhanced accessibility to advanced language technologies across the region.</p><p>Sailor2 builds upon the foundation of the awesome multilingual model <strong>Qwen2.5</strong> (<a href=https://qwenlm.github.io/blog/qwen2.5-llm/>Learn more here</a>) and is continuously pre-trained on ~500B tokens to support 15 languages better with a unified model. These languages include: English, Chinese, Burmese ðŸ‡²ðŸ‡², CebuanoðŸ‡µðŸ‡­, IlocanoðŸ‡µðŸ‡­, IndonesianðŸ‡®ðŸ‡©, JavaneseðŸ‡®ðŸ‡©, KhmerðŸ‡°ðŸ‡­, LaoðŸ‡±ðŸ‡¦, MalayðŸ‡²ðŸ‡¾, SundaneseðŸ‡®ðŸ‡©, TagalogðŸ‡µðŸ‡­, ThaiðŸ‡¹ðŸ‡­, VietnameseðŸ‡»ðŸ‡³ and WarayðŸ‡µðŸ‡­.</p><p>By addressing the growing demand for diverse, robust, and accessible language models, Sailor2 seeks to <em>serve the underserved</em> in SEA areas with open, inclusive, and accessible multilingual LLMs.</p><h1 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h1><p>We evaluate our models on several benchmarks, including <a href=https://arxiv.org/abs/2404.01854>IndoCutlure</a>, <a href=https://arxiv.org/abs/2003.05002>TydiQA</a>, Meta Thai MMLU, <a href=https://arxiv.org/abs/2306.05179>M3Exam</a>, VMLU, Tatabahasa, FLORES-200, and XCOPA. The results of TydiQA, M3Exam, FLORES-200 and XCOPA are obtained from our previously released evaluation suite <a href=https://github.com/sail-sg/sailcompass>SailCompass</a>. The results of Meta Thai MMLU are from the <a href=https://github.com/huggingface/lighteval>LightEval</a>, and others are obtained from the evaluation scripts shared by the community members. More evaluation details will be shared in our upcoming paper.</p><p><img loading=lazy src=/img/sailor2/sailor2_20b_base.jpg alt="20B Performance"></p><p>Compared to other advanced multilingual models like Qwen2.5-32B, Gemma2-27B, Llama3.1-70B, and Aya-Expanse-32B, our flagship 20B model demonstrates comparable or superior performance on languages such as Indonesian, Thai, Vietnamese, Malay, and Javanese. It excels in machine translation tasks and multilingual reasoning, including benchmarks like XCOPA. Most notably, due to the inclusive design of the Sailor2 models, our 20B model significantly outperforms others in extremely low-resource languages, such as M3Exam-Javanese, achieving an improvement of +14.6 over Qwen2.5-32B.</p><p><img loading=lazy src=/img/sailor2/sailor2_8b_base.jpg alt="8B Performance"></p><p>The Sailor2-8B model stands out as the best multilingual model for SEA languages in the &lt; 10B category, outperforming all other open-access language models in general.</p><p><img loading=lazy src=/img/sailor2/sailor2_chat_perf.jpg alt="Chat Performance"></p><p>We also evaluated our chat models using the translated WildBench, <a href=https://huggingface.co/datasets/sailor2/sea-wildbench>Sea-WildBench</a>, a challegning benchmark for chat models. As shown, the win rate of Sailor2-20B-Chat against GPT-4o-0806 on SeaWildBench is nearly 50%, demonstrating that Sailor2-20B-Chat performs at a GPT-4o level for local chat scenarios.</p><h2 id=continual-pre-training>Continual Pre-training<a hidden class=anchor aria-hidden=true href=#continual-pre-training>#</a></h2><p>Our data is primarily sourced from web HTML documents and publicly available PDF files, covering a diverse range of languages, including English, Chinese, and the above mentioned SEA languages. For English and Chinese datasets, we predominantly utilize existing datasets shared by the community. For SEA languages, we collect the data independently and in collaboration with partners.</p><p>The dataset for SEA languages continual pre-training, sourcing documents from a variety of materials containing up-to-date knowledge until the end of 2023. To ensure high data quality, we apply several deduplication and cleaning techniques across each source. After comprehensive deduplication and cleaning, the total available dataset is as follows, along with its corresponding disk usage (ordered by disk size):</p><table><thead><tr><th style=text-align:center>Language</th><th style=text-align:center>Disk size</th></tr></thead><tbody><tr><td style=text-align:center>Vietnamese</td><td style=text-align:center>1.9T</td></tr><tr><td style=text-align:center>Indonesian</td><td style=text-align:center>1.3T</td></tr><tr><td style=text-align:center>Thai</td><td style=text-align:center>242G</td></tr><tr><td style=text-align:center>Malay</td><td style=text-align:center>44G</td></tr><tr><td style=text-align:center>Burmese</td><td style=text-align:center>25.8G</td></tr><tr><td style=text-align:center>Tagalog</td><td style=text-align:center>17.5G</td></tr><tr><td style=text-align:center>Khmer</td><td style=text-align:center>6.9G</td></tr><tr><td style=text-align:center>Cebuano</td><td style=text-align:center>2.1G</td></tr><tr><td style=text-align:center>Lao</td><td style=text-align:center>1.9G</td></tr><tr><td style=text-align:center>Javanese</td><td style=text-align:center>1.2G</td></tr><tr><td style=text-align:center>Waray</td><td style=text-align:center>0.8G</td></tr><tr><td style=text-align:center>Sundanese</td><td style=text-align:center>0.75G</td></tr><tr><td style=text-align:center>Ilocano</td><td style=text-align:center>0.2G</td></tr></tbody></table><h2 id=model-expansion>Model Expansion<a hidden class=anchor aria-hidden=true href=#model-expansion>#</a></h2><p>The Sailor2 model comes in three sizes, 1B, 8B, and 20B, which are expanded from the Qwen2.5 base models of 0.5B, 7B, and 14B, respectively. The decision was made to perform model expansion prior to continual pre-training in order to mitigate the potential for forgetting of English and Chinese language capabilities, while also enhancing the model&rsquo;s capacity for further improvements in SEA languages.</p><p>In practice, the approach draws inspiration from the method proposed by <a href=https://arxiv.org/abs/2401.02415>LlamaPro</a>, leveraging a block-expansion mechanism in the original Qwen2.5 model. This approach significantly enhances the model&rsquo;s performance in SEA languages while maintaining stable capabilities in English and Chinese. By utilizing the strategy, the newly introduced layers are able to store the additional SEA knowledge from the continually pre-trained tokens, rather than overwriting the existing linguistic information of the other languages.</p><h2 id=data-cleaning-and-deduplication>Data Cleaning and Deduplication<a hidden class=anchor aria-hidden=true href=#data-cleaning-and-deduplication>#</a></h2><p>We leveraged <a href=https://github.com/sail-sg/sailcraft>sailcraft</a> for comprehensive data processing, employing multiple deduplication techniques including near and exact deduplication, heuristics-based cleaning, URL deduplication, and frequent line removal. During URL deduplication, we prioritized documents with more content, effectively reducing total tokens by nearly 50%. As for the frequent line removal, following the <a href=https://arxiv.org/abs/2407.21783>Llama3 approach</a>, we removed lines appearing more than 5 times in 10M document buckets, successfully eliminating nearly 5% of total tokensâ€”most of which were determined to be meaningless content.</p><h2 id=two-stage-training>Two-Stage Training<a hidden class=anchor aria-hidden=true href=#two-stage-training>#</a></h2><p>We adopt the two-stage pre-training approach introduced in the <a href=/>MiniCPM paper</a>. In the first stage, we use comprehensive datasets and a relatively high learning rate (<code>1e-4</code>), while in the second stage, we focus on high-quality tokens with a smaller learning rate (<code>1e-5</code>).</p><p>Drawing from the forgetting rules outlined in the Sailor paper, we introduce high-resource languages during the first stage (such as English, Chinese, Vietnamese, Indonesian, Thai, Malay, Burmese, Tagalog, and Khmer). In the second stage, we transition to both high-resource and low-resource languages (including Cebuano, Lao, Javanese, Waray, Sundanese, and Ilocano). This two-stage approach allows automatic data mixture in the first stage, while allowing us to incorporate high-quality tokens from low-resource languages in the second stage without rescheduling the mixing ratios.</p><h3 id=stage-1-balanced-data-mixture>Stage 1: Balanced Data Mixture<a hidden class=anchor aria-hidden=true href=#stage-1-balanced-data-mixture>#</a></h3><p>In the stage 1 of our continual pre-training, we select a subset of languages that could provide sufficiently enough tokens for data mixture optimization. We employed RegMix to optimize the data mixture, with the primary objective of maximizing the log sum across all languages considered in stage 1.</p><p>Unlike our previous practices in <a href=https://arxiv.org/abs/2404.03608>Sailor</a> that used 0.5B models as proxy models for data mixture, we follow <a href=https://arxiv.org/abs/2407.01492>RegMix</a> to utilize 1M models as our proxy model, even for the scenario of continual pre-training. Our underlying assumption was that if a model can be trained over an extended period, the converged or equivalent data mixture should remain relatively consistent.</p><p>After conducting 1,000 runs of data mixture optimization using 1M models, we observed a subtle shift from the original token distribution. Notably, the optimized data mixture resulted in upsampling languages like Khmer, Malay, Burmese, Thai, and Tagalog, while simultaneously downsampling Indonesian and Vietnamese. The final data mixture of Stage 1 is as below (tokens counted in the tokenizer of Qwen2.5):</p><table><thead><tr><th style=text-align:center>Language</th><th style=text-align:center>Effective Tokens</th></tr></thead><tbody><tr><td style=text-align:center>Vietnamese</td><td style=text-align:center>102B</td></tr><tr><td style=text-align:center>Indonesian</td><td style=text-align:center>94B</td></tr><tr><td style=text-align:center>Thai</td><td style=text-align:center>92B</td></tr><tr><td style=text-align:center>English</td><td style=text-align:center>51B</td></tr><tr><td style=text-align:center>Chinese</td><td style=text-align:center>50B</td></tr><tr><td style=text-align:center>Burmese</td><td style=text-align:center>23.5B</td></tr><tr><td style=text-align:center>Malay</td><td style=text-align:center>21B</td></tr><tr><td style=text-align:center>Tagalog</td><td style=text-align:center>10B</td></tr><tr><td style=text-align:center>Khmer</td><td style=text-align:center>6.5B</td></tr><tr><td style=text-align:center><strong>Stage 1 (Total)</strong></td><td style=text-align:center><strong>450B</strong></td></tr></tbody></table><h3 id=stage-2-synthetic-data-to-retrieve-high-quality-tokens>Stage 2: Synthetic Data to Retrieve High-Quality Tokens<a hidden class=anchor aria-hidden=true href=#stage-2-synthetic-data-to-retrieve-high-quality-tokens>#</a></h3><p>In stage 2, we lower the learning rate to <code>1e-5</code> (1/10 of the original learning rate), and take 20% of the stage 1 dataset to make sure the model still behaves well on the original distribution. As for the remaining 80% training budget, we allocate them to high-quality SEA tokens, where all low-resource languages are added, and the token distribution of high-resource languages is maintained as similar to the stage 1.</p><p>To address challenges in selecting high-quality datasets for low-resource languages, we use NLLB 3.3B to translate high-quality English documents into local languages. A fast-text classifier is then trained for each language to identify high-quality subsets, using 10K positive and 10K negative examples. Positive examples are machine-translated from English high-quality datasets, while negative examples are sampled from the raw dataset from the web. After training, we rank the dataset by classifier scores and select the top results (between 10% to 20%, according to the requirement of the ratio in data mixture) as the high-quality subset for the stage 2 training. In addition, we also added some English instruction tuning datasets and some datasets contributed by the Sailor2 community.</p><table><thead><tr><th style=text-align:center>Language</th><th style=text-align:center>Effective Tokens</th></tr></thead><tbody><tr><td style=text-align:center>Stage 1</td><td style=text-align:center>10B</td></tr><tr><td style=text-align:center>English Instruction Tuning Dataset</td><td style=text-align:center>2.5B</td></tr><tr><td style=text-align:center>Vietnamese (High-Quality)</td><td style=text-align:center>10.9B</td></tr><tr><td style=text-align:center>Indonesian (High-Quality)</td><td style=text-align:center>12.8B</td></tr><tr><td style=text-align:center>Thai (High-Quality)</td><td style=text-align:center>13.9B</td></tr><tr><td style=text-align:center>Burmese (High-Quality)</td><td style=text-align:center>2.8B</td></tr><tr><td style=text-align:center>Malay (High-Quality)</td><td style=text-align:center>1.3B</td></tr><tr><td style=text-align:center>Tagalog (High-Quality)</td><td style=text-align:center>2.2B</td></tr><tr><td style=text-align:center>Khmer (High-Quality)</td><td style=text-align:center>0.9B</td></tr><tr><td style=text-align:center>Waray (High-Quality)</td><td style=text-align:center>0.02B</td></tr><tr><td style=text-align:center>Ilocano (High-Quality)</td><td style=text-align:center>0.05B</td></tr><tr><td style=text-align:center>Javanese (High-Quality)</td><td style=text-align:center>0.17B</td></tr><tr><td style=text-align:center>Lao (High-Quality)</td><td style=text-align:center>0.33B</td></tr><tr><td style=text-align:center>Cebuano (High-Quality)</td><td style=text-align:center>0.30B</td></tr><tr><td style=text-align:center>Sundanese (High-Quality)</td><td style=text-align:center>0.09B</td></tr><tr><td style=text-align:center><strong>Stage 2 (Total)</strong></td><td style=text-align:center><strong>60B</strong></td></tr></tbody></table><h2 id=post-training>Post-Training<a hidden class=anchor aria-hidden=true href=#post-training>#</a></h2><p>We conduct both the supervised fine-tuning and preference tuning based on the above Sailor2 base models.</p><h3 id=supervised-fine-tuning>Supervised Fine-tuning<a hidden class=anchor aria-hidden=true href=#supervised-fine-tuning>#</a></h3><p>Drawing inspiration from <a href=https://opencoder-llm.github.io/>OpenCoder</a>, we have implemented a strategic two-stage Supervised Fine-Tuning (SFT) process that tackles common training challenges head-on. In the first stage, we cast a wide net, training the model on the entire dataset for a single epoch using large batch sizes. This initial exposure helps establish broad coverage across different SEA languages and domains.</p><p>The second stage switches to a more focused approach with smaller batch sizes and multiple training epochs on carefully curated, high-quality data that is balanced across domains and languages. Our quality selection process relies on two key metrics: perplexity from the Sailor2 base model (PPL) and reward scores. High-perplexity examples represent cases where the model is less confident â€“ precisely the areas where it needs the most improvement. While high perplexity identifies learning opportunities, high reward scores ensure we&rsquo;re learning from exemplary content.</p><p>To prevent the model from overfitting to similar examples, we employ embedding-based similarity checks. This additional filter ensures our training data remains diverse and representative, leading to a more robust and versatile model. This refined approach to fine-tuning has proven effective in producing models that are both broadly capable and specifically strong in high-priority domains.</p><h3 id=preference-tuning>Preference Tuning<a hidden class=anchor aria-hidden=true href=#preference-tuning>#</a></h3><p>Our preference tuning consists of two stages, training with off-policy responses generated by Llama-3-8B-Instruct and training with on-policy responses generated by Sailor2 suite. In the off-policy training stage, we first translate the preference dataset, <a href=https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback-armorm>UF-Llama3</a>, into SEA languages. Low-quality translations are then filtered based on perplexity scores provided by the Sailor2-8B base. The resulting off-policy dataset is a mixture of SEA languages and English. In the on-policy training stage, we utilize the prompts from the off-policy dataset. For each prompt, responses are generated using the model trained on the off-policy data. These responses are rated by the open-source reward model, <a href=https://huggingface.co/Skywork/Skywork-Reward-Gemma-2-27B>Skywork-Reward-Gemma-2-27B</a>. The highest-rated response is selected as the chosen response and the lowest-rated response is selected as the rejected response. Additionally, we employ a language-consistency verifier to detect and correct cases where the input language differs from the output language, except for the translation task.</p><p><img loading=lazy src=/img/sailor2/sailor2-sft-dpo.jpg alt=sailor2-sft-dpo></p><p>Due to the absence of a high-quality reward model for PPO-based algorithms, we explore different direct alignment algorithms, such as DPO and its variants. We conducted extensive hyperparameter tuning and ablation studies to ensure the model performance on the development evaluations. All experiments were conducted with the training framework, <a href=https://github.com/sail-sg/oat>Oat</a>, which enables large-scale and flexible training. Below, we summarize our key findings:</p><ul><li>DPO achieves comparable performance to its length-normalized variants (e.g. Length-normalized DPO and SimPO) while demonstrating better training stability.</li><li>Off-policy training serves as an effective initialization for on-policy training.</li><li>On-policy training further improves the model performance across various benchmarks.</li><li>The dataset processed using the language-consistency verifier provides a consistent performance gain.</li></ul><h2 id=develop-with-sailor2>Develop with Sailor2<a hidden class=anchor aria-hidden=true href=#develop-with-sailor2>#</a></h2><p>The code of Sailor2 has been in the latest Hugging face transformers and we advise you to install <code>transformers==4.46.3</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s2>&#34;cuda&#34;</span> <span class=c1># the device to load the model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;sail/Sailor2-20B&#34;</span><span class=p>,</span> <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;sail/Sailor2-20B&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>input_message</span> <span class=o>=</span> <span class=s2>&#34;Model bahasa adalah model probabilistik&#34;</span> 
</span></span><span class=line><span class=cl><span class=c1># The given Indonesian input translates to &#39;A language model is a probabilistic model of.&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>input_message</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>64</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>input_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>output_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span><span class=n>generated_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=limited-free-api-service>Limited Free API Service<a hidden class=anchor aria-hidden=true href=#limited-free-api-service>#</a></h2><p>We also offer a limited free API service for Sailor2-20B-Chat in collaboration with <a href=https://blog.float16.cloud/>Float16.cloud</a>. You can access the API using the following details:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>curl -X POST <span class=s2>&#34;https://api.float16.cloud/v1/chat/completions&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -H <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -H <span class=s2>&#34;Authorization: Bearer sailor2&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -d <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;model&#34;: &#34;sailor2-20b-chat&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;messages&#34;: [
</span></span></span><span class=line><span class=cl><span class=s1>      {
</span></span></span><span class=line><span class=cl><span class=s1>        &#34;role&#34;: &#34;system&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>        &#34;content&#34;: &#34;You are an AI assistant named Sailor2, created by Sea AI Lab. As an AI assistant, you can answer questions in English, Chinese, and Southeast Asian languages such as Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. Your responses should be friendly, unbiased, informative, detailed, and faithful.&#34;
</span></span></span><span class=line><span class=cl><span class=s1>      },
</span></span></span><span class=line><span class=cl><span class=s1>      {
</span></span></span><span class=line><span class=cl><span class=s1>        &#34;role&#34;: &#34;user&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>        &#34;content&#34;: &#34;à¸ªà¸§à¸±à¸ªà¸”à¸µ&#34;
</span></span></span><span class=line><span class=cl><span class=s1>      }
</span></span></span><span class=line><span class=cl><span class=s1>    ]
</span></span></span><span class=line><span class=cl><span class=s1>  }&#39;</span>
</span></span></code></pre></div><h2 id=acknowledgement>Acknowledgement<a hidden class=anchor aria-hidden=true href=#acknowledgement>#</a></h2><p><img loading=lazy src=/img/sailor2/sailor2_banner.jpg alt="sailor2 banner"></p><p>This is a huge effort from the collaboration among the community, along with researchers from Sea AI Lab, SCB10X, WiseSight, HuggingFace, and the whole Sailor2 community.</p><h2 id=limitations-and-next-steps>Limitations and Next Steps<a hidden class=anchor aria-hidden=true href=#limitations-and-next-steps>#</a></h2><p>Currently, Sailor2 supports a context length of up to 4K tokens. Extending support for longer context lengths, particularly for multilingual use cases, remains a challenge. We are actively working on improvements and plan to support longer contexts in the near future.</p><p>Additionally, due to the complexity of handling multiple languages simultaneously, Sailor2 may switch languages during responses, especially for smaller ones. We will investigate the problems further.</p></div></article></main><footer class=footer><span>&copy; 2024 <a href=http://sea-sailor.github.io/>Sailor</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span>Acknowledgment to
<a href=https://qwenlm.github.io/ rel="noopener noreferrer" target=_blank>Qwen</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>