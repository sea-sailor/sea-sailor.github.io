<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Resources | Sailor</title>
<meta name=keywords content><meta name=description content='Links
In the following, we provide important links for you to refer to our opensource resources.
PAPER
GITHUB
HUGGING FACE
Quick Start
It is simple to use Sailor models through Hugging Face Transformers. Below is a demo usage for a quick start:
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
device = "cuda"

model = AutoModelForCausalLM.from_pretrained(
    &#39;sail/Sailor2-20B-Chat&#39;,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(&#39;sail/Sailor2-20B-Chat&#39;)
system_prompt= \
&#39;You are an AI assistant named Sailor2, created by Sea AI Lab. \
As an AI assistant, you can answer questions in English, Chinese, and Southeast Asian languages \
such as Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. \
Your responses should be friendly, unbiased, informative, detailed, and faithful.&#39;

prompt = "Beri saya pengenalan singkat tentang model bahasa besar."
# prompt = "Hãy cho tôi một giới thiệu ngắn gọn về mô hình ngôn ngữ lớn."
# prompt = "ให้ฉันแนะนำสั้น ๆ เกี่ยวกับโมเดลภาษาขนาดใหญ่"

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(device)
input_ids = model_inputs.input_ids.to(device)

generated_ids = model.generate(
    input_ids,
    max_new_tokens=512,
)

generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
'><meta name=author content="Sailor Team"><link rel=canonical href=http://sea-sailor.github.io/resources/><link crossorigin=anonymous href=/assets/css/stylesheet.5535818ba8080a84fafce624766afd4112e9f8489b7a167457c88f8ea1398972.css integrity="sha256-VTWBi6gICoT6/OYkdmr9QRLp+EibehZ0V8iPjqE5iXI=" rel="preload stylesheet" as=style><link rel=icon href=http://sea-sailor.github.io/img/sailor_logo_trans.png><link rel=icon type=image/png sizes=16x16 href=http://sea-sailor.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://sea-sailor.github.io/favicon-32x32.png><link rel=apple-touch-icon href=http://sea-sailor.github.io/apple-touch-icon.png><link rel=mask-icon href=http://sea-sailor.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://sea-sailor.github.io/resources/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BP9G7L138H"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BP9G7L138H")}</script><meta property="og:title" content="Resources"><meta property="og:description" content='Links
In the following, we provide important links for you to refer to our opensource resources.
PAPER
GITHUB
HUGGING FACE
Quick Start
It is simple to use Sailor models through Hugging Face Transformers. Below is a demo usage for a quick start:
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
device = "cuda"

model = AutoModelForCausalLM.from_pretrained(
    &#39;sail/Sailor2-20B-Chat&#39;,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(&#39;sail/Sailor2-20B-Chat&#39;)
system_prompt= \
&#39;You are an AI assistant named Sailor2, created by Sea AI Lab. \
As an AI assistant, you can answer questions in English, Chinese, and Southeast Asian languages \
such as Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. \
Your responses should be friendly, unbiased, informative, detailed, and faithful.&#39;

prompt = "Beri saya pengenalan singkat tentang model bahasa besar."
# prompt = "Hãy cho tôi một giới thiệu ngắn gọn về mô hình ngôn ngữ lớn."
# prompt = "ให้ฉันแนะนำสั้น ๆ เกี่ยวกับโมเดลภาษาขนาดใหญ่"

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(device)
input_ids = model_inputs.input_ids.to(device)

generated_ids = model.generate(
    input_ids,
    max_new_tokens=512,
)

generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
'><meta property="og:type" content="article"><meta property="og:url" content="http://sea-sailor.github.io/resources/"><meta property="og:image" content="http://sea-sailor.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content><meta property="og:site_name" content="Sailor"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://sea-sailor.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Resources"><meta name=twitter:description content='Links
In the following, we provide important links for you to refer to our opensource resources.
PAPER
GITHUB
HUGGING FACE
Quick Start
It is simple to use Sailor models through Hugging Face Transformers. Below is a demo usage for a quick start:
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
device = "cuda"

model = AutoModelForCausalLM.from_pretrained(
    &#39;sail/Sailor2-20B-Chat&#39;,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(&#39;sail/Sailor2-20B-Chat&#39;)
system_prompt= \
&#39;You are an AI assistant named Sailor2, created by Sea AI Lab. \
As an AI assistant, you can answer questions in English, Chinese, and Southeast Asian languages \
such as Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. \
Your responses should be friendly, unbiased, informative, detailed, and faithful.&#39;

prompt = "Beri saya pengenalan singkat tentang model bahasa besar."
# prompt = "Hãy cho tôi một giới thiệu ngắn gọn về mô hình ngôn ngữ lớn."
# prompt = "ให้ฉันแนะนำสั้น ๆ เกี่ยวกับโมเดลภาษาขนาดใหญ่"

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(device)
input_ids = model_inputs.input_ids.to(device)

generated_ids = model.generate(
    input_ids,
    max_new_tokens=512,
)

generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Resources","item":"http://sea-sailor.github.io/resources/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Resources","name":"Resources","description":"Links In the following, we provide important links for you to refer to our opensource resources.\nPAPER GITHUB HUGGING FACE\nQuick Start It is simple to use Sailor models through Hugging Face Transformers. Below is a demo usage for a quick start:\nimport torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; model = AutoModelForCausalLM.from_pretrained( \u0026#39;sail/Sailor2-20B-Chat\u0026#39;, torch_dtype=torch.bfloat16, device_map=\u0026#34;auto\u0026#34; ) tokenizer = AutoTokenizer.from_pretrained(\u0026#39;sail/Sailor2-20B-Chat\u0026#39;) system_prompt= \\ \u0026#39;You are an AI assistant named Sailor2, created by Sea AI Lab. \\ As an AI assistant, you can answer questions in English, Chinese, and Southeast Asian languages \\ such as Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. \\ Your responses should be friendly, unbiased, informative, detailed, and faithful.\u0026#39; prompt = \u0026#34;Beri saya pengenalan singkat tentang model bahasa besar.\u0026#34; # prompt = \u0026#34;Hãy cho tôi một giới thiệu ngắn gọn về mô hình ngôn ngữ lớn.\u0026#34; # prompt = \u0026#34;ให้ฉันแนะนำสั้น ๆ เกี่ยวกับโมเดลภาษาขนาดใหญ่\u0026#34; messages = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: system_prompt}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) model_inputs = tokenizer([text], return_tensors=\u0026#34;pt\u0026#34;).to(device) input_ids = model_inputs.input_ids.to(device) generated_ids = model.generate( input_ids, max_new_tokens=512, ) generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] print(response) ","keywords":[],"articleBody":"Links In the following, we provide important links for you to refer to our opensource resources.\nPAPER GITHUB HUGGING FACE\nQuick Start It is simple to use Sailor models through Hugging Face Transformers. Below is a demo usage for a quick start:\nimport torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \"cuda\" model = AutoModelForCausalLM.from_pretrained( 'sail/Sailor2-20B-Chat', torch_dtype=torch.bfloat16, device_map=\"auto\" ) tokenizer = AutoTokenizer.from_pretrained('sail/Sailor2-20B-Chat') system_prompt= \\ 'You are an AI assistant named Sailor2, created by Sea AI Lab. \\ As an AI assistant, you can answer questions in English, Chinese, and Southeast Asian languages \\ such as Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. \\ Your responses should be friendly, unbiased, informative, detailed, and faithful.' prompt = \"Beri saya pengenalan singkat tentang model bahasa besar.\" # prompt = \"Hãy cho tôi một giới thiệu ngắn gọn về mô hình ngôn ngữ lớn.\" # prompt = \"ให้ฉันแนะนำสั้น ๆ เกี่ยวกับโมเดลภาษาขนาดใหญ่\" messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) model_inputs = tokenizer([text], return_tensors=\"pt\").to(device) input_ids = model_inputs.input_ids.to(device) generated_ids = model.generate( input_ids, max_new_tokens=512, ) generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] print(response) ","wordCount":"201","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Sailor Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://sea-sailor.github.io/resources/"},"publisher":{"@type":"Organization","name":"Sailor","logo":{"@type":"ImageObject","url":"http://sea-sailor.github.io/img/sailor_logo_trans.png"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Sailor (Alt + H)"></a><div class=logo-switches></div></div><ul id=menu><li><a href=/ title=Home><span>Home</span></a></li><li><a href=/resources title=Resources><span class=active>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(to top,#6a11cb 0%,#2575fc 100%)"></div><div class="hero text-light"><h1 class=post-title>Resources</h1></div></div><main class=main><article class=post-single><div class=post-content><h2 id=links>Links<a hidden class=anchor aria-hidden=true href=#links>#</a></h2><p>In the following, we provide important links for you to refer to our opensource resources.</p><p><a href=https://arxiv.org/abs/2404.03608 class="btn external" target=_blank>PAPER</a>
<a href=https://github.com/sail-sg/sailor2 class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/collections/sail/sailor2-language-models-674d7c9e6b4dbbd9a869906b class="btn external" target=_blank>HUGGING FACE</a></p><h2 id=quick-start>Quick Start<a hidden class=anchor aria-hidden=true href=#quick-start>#</a></h2><p>It is simple to use Sailor models through Hugging Face Transformers. Below is a demo usage for a quick start:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s2>&#34;cuda&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;sail/Sailor2-20B-Chat&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;sail/Sailor2-20B-Chat&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>system_prompt</span><span class=o>=</span> \
</span></span><span class=line><span class=cl><span class=s1>&#39;You are an AI assistant named Sailor2, created by Sea AI Lab. </span><span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=s1>As an AI assistant, you can answer questions in English, Chinese, and Southeast Asian languages </span><span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=s1>such as Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. </span><span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=s1>Your responses should be friendly, unbiased, informative, detailed, and faithful.&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;Beri saya pengenalan singkat tentang model bahasa besar.&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># prompt = &#34;Hãy cho tôi một giới thiệu ngắn gọn về mô hình ngôn ngữ lớn.&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># prompt = &#34;ให้ฉันแนะนำสั้น ๆ เกี่ยวกับโมเดลภาษาขนาดใหญ่&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>system_prompt</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>text</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>input_ids</span> <span class=o>=</span> <span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>input_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>output_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span><span class=n>generated_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2025 <a href=http://sea-sailor.github.io/>Sailor</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span>Acknowledgment to
<a href=https://qwenlm.github.io/ rel="noopener noreferrer" target=_blank>Qwen</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>